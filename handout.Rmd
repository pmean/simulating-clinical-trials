---
title: "Simulating clinical trials"
author: "Steve Simon"
date: "May 25, 2017"
output: html_document
---

# Simulation before, during, and after a clinical trial: A Bayesian approach

Steve Simon, mail@pmean.com

The code that produced these figures is available at https://github.com/pmean/simulating-clinical-trials

Thanks to my collaborators: Byron Gajewski, Jiang Yu

Thanks to Vladimir Anisimov for inviting me

Thanks to the people of Great Britain for providing such a nice venue for this conference.

## Abstract

Simulation of a clinical trial gives you answers to important economic, logistical, or scientific questions about the trial when some of the features are difficult to characterize with perfect precision. A Bayesian approach with informative priors offers a flexible framework for trial simulation. It provides a seamless transition from simulation prior to the trial to simulation during the trial itself. Although informative priors are controversial, you can avoid perceptions of bias by restricting the informative priors to clinical trial features that are independent of your research hypothesis. You can protect your interim predictions against unrealistic prior beliefs by implementing the hedging hyperprior, a simple hyperdistribution that downweights the strength of the prior when there is a discrepancy between the prior distribution and data observed during the trial itself. The Bayesian approach also gives you a simple post mortem analysis after the trial ends. You can compute percentile values by plugging the point estimates from the actual clinical trial data into the corresponding prior distributions. Over multiple trials, a deviation in these percentiles from a uniform distribution indicates biased specification of the informative priors. The Bayesian approach to trial simulation will be illustrated using various patient accrual models.

Keywords: hedging hyperprior; informative prior distributions; Markov Chain Monte Carlo; patient accrual.

## Introduction.

"A Bayesian is one who, vaguely expecting a horse, and catching a glimpse of a donkey, strongly believes he has seen a mule." --Stephen Senn

The fundamental approach in Bayesian data analysis is combining information from a prior distribution with information from the data. The posterior mean is a weighted average of the prior mean and the mean of the data. This is only true when you use an informative prior. When you use a non-informative prior (sometimes called a flat prior), the posterior mean is pretty much equal to the mean of the data.

The use of informative priors in testing efficacy and safety is controversial, but this should not stop you from using informative priors in monitoring the operational characteristics of a clinical trial.

## Part 1. An illustration of various simulations

```{r key-data, echo=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(
  echo=FALSE,
  message=FALSE,
  warning=FALSE)

# load constants used throughout the various programs.
load("fig/0.0.Rdata")
```

Consider a clinical trial that plans to run for T = `r T` days (`r T/365` years). You hope to recruit N = `r N` patients in that time, which would mean `r round(N/T, 2)` patients per day or `r round(30*N/T, 1)` patients per month. You suspect, however, that the accrual rate might actually be quite a bit higher or quite a bit lower than this target. You set a prior distribution on the accrual rate that is Gamma(`r N*S`, `r T*S`). You need to wait for an explanation of why this might be a reasonable prior distribution. With this prior distribution, you can simulate many things.

## Figure 1.1. Simulation of a clinical trial with a fixed sample size.

```{r simulate-time, fig.width=10, fig.height=5, eval=TRUE}
load("fig/1.1.Rdata")
print(fig)
```

This is simulation of the amount of time it would take to recruit `r N` patients. The target is `r T` days, but some simulations take less time and others take more time.

## Figure 1.2. Simulation of a clinical trial with a fixed time frame.

```{r simulate-n, fig.width=10, fig.height=5}
load("fig/1.2.RData")
print(fig)
```

This is simulation of the number of patients you can recruit if you restrict the amount of time spentt to exactly `r T` days. The target is `r N` patients, but some simulations estimate a larger or a smaller number of patients.

## Figure 1.3. Simulation of a trial with a conditional endpoint.

```{r simulate-composite, fig.width=10, fig.height=5, eval=TRUE}
load("fig/1.3.RData")
print(fig)
```

This is a trial with a conditional endpoint. You will end the trial when you get `r N` patients or your trial takes `r T` days, whichever comes first.

## Figure 1.4. Simulation of the cost of a clinical trial. 

```{r simulate-cost, fig.width=10, fig.height=5, eval=TRUE}
load("fig/1.4.RData")
print(fig)
```

This is the same conditional endpoint, but you are measuring the cost of the trial rather than the number of patients or the amount of time. The cost is 80 British Pounds for each day of the trial and 20 British Pounds for each patient in the trial.

## Part 2.  Paths in a simulation

When you simulate a clinical trial, you won't do this two times. You'll do it several thousand times, because computer cycles are cheap. You can run these simulations before the trial starts, but you can continue to run them during the trial itself. You should even run a simulation after the trial ends, comparing the actual trial results with what you thought they might be prior to the start of the trial. Consider this a sort of "post mortem" analysis.

## Figure 2.1. 1000 simulations run prior to start of a clinical trial.

```{r path1, fig.width=10, fig.height=5}
load("fig/2.1.RData")
print(fig)
```

There is a fairly wide spread of possible sample sizes in this trial.

## Figure 2.2. 1000 simulations run during a clinical trial.

```{r path2, fig.width=10, fig.height=5}
load("fig/2.2.RData")
print(fig)
```

You an also run a simulation of a clinical trial during the trial. What this means is that you have information sample size up to a certain point in time and you want to simulate from that information and from your prior knowledge of accrual rates what the final sample size might be.

## Figure 2.3. Post mortem examination of actual trial results.

```{r path3, fig.width=10, fig.height=5}
load("fig/2.3.RData")
print(fig)
```

When the trial ends, you can compare the actual accrual results to what you though
t you knew before the trial started. In this particular trial, some of the interim values are made up, but it reflects a trial that ended up short of the target sample size (`r n_final` instead of `r N` patients) in spite of the fact that it needed `r t_final` days instead of `r T`. Your prior distribution was overly optimistic.

## Part 3. Gamma Poisson models

From this point onward, you'll be seeing summarizations of the simulations using boxplots and scatterplots, but it's worth remembering that behind these graphs are thousands of random paths. The simulation of total sample size for a fixed time frame is a Poisson model with an informative gamma prior. 

## Figure 3.1. Boxplot of prior distribution for accrual rate.

```{r gamma-poisson-prior, fig.width=10, fig.height=1, eval=TRUE}
load("fig/3.1a.RData"); print(fig)
load("fig/3.1b.RData"); print(fig)
load("fig/3.1c.RData"); print(fig)
```

Selecting a prior distribution requires a lot of work. It helps, I believe to express the strength of the prior distribution as a fraction of the planned sample size. A prior with a strength equivalent to 40% of the planned sample size represents a setting you give equal weight to the prior distribution and the data when you have collected 40% of the data. That's a very strong prior, but not unreasonable. As a researcher who has done this sort of clinical trial many times, the value of S=`r S` seems like the best choice to you.

Some of you may be cautious about such a bold prior, so for perspective, I am showing (in gray) two weaker prior distributions.

The tick marks on these graphs and all future graphs are placed at the 1st, 25th, 50th, 75th, and 99th percentiles.

## Stan code for the gamma-Poisson model

```{r gamma-poisson-code, eval=TRUE}
f <- "3-gamma-poisson.stan"
cat(readLines(f), sep="\n")
```

Here is code in Stan (running inside of R) that simulates the results of a clinical trial with a target goal of `r N` patients in ` r round(T/365, 1)` years (`r T` days) and places a range of uncertainty on the accrual rate that is characterized by a gamma(`r N*S`, `r T*S`). This is a prior distribution with a strength roughly equal to `r 100*S`% of the target sample size.

## Figure 3.2. Simulated total sample size prior to start of clinical trial.

```{r gamma-poisson-prior-predictions, fig.width=10, fig.height=1}
load("fig/3.2.RData")
print(fig)
```

The center of the boxplot represents the median sample size from the simulation, and it is close to our target. The spread in the boxplot is caused by the random nature of patient accrual and uncertainty associated with the Poisson rate parameter.

## Figure 3.3. Relationship of prior estimate of accrual rate and estimated total sample size.

```{r gamma-poisson-prior-scatterplot, fig.width=10, fig.height=5, eval=TRUE}
load("fig/3.3.RData")
print(fig)
```

You can learn a lot by looking at how the estimated total sample size relates to the prior paramter(s). 

Recall that in this simulation, you randomly select an accrual rate, gamma, and then you simulate a path based on that value of lambda. Not too surprisingly, the sample size depends on what lambda is chosen, though there is some variation, even for the same value of lambda.

## Figure 3.4. Relationship of fixed accrual estimate and estimated total sample size.

```{r gamma-poisson-prior-fixed, fig.width=10, fig.height=5, eval=TRUE}
load("fig/3.4.RData")
print(fig)
```

With a plot like this, you can run some sensitivity checks, such as "what would happen if the accrual rate were closer to 8.5 patients per month"

## Update

The key reason that you should run your simulations in a Bayesian framework is that you can make a seamless transition to a simulation of a clinical trial during the trial itself. In this trial, the early accrual rate was much lower than expected. After `r t` days, you have only gotten `r n` patients. If you were on target, it would have taken only `r T` * (`r n` / `r N`) = `r round(T*(n/N))` days to get this many patients. How much is this shortfall hurting you?

## Figure 3.5. Simulated total sample size during the clinical trial.

```{r gamma-poisson-update, fig.width=10, fig.height=1}
load("fig/3.5a.RData"); print(fig)
load("fig/3.5b.RData"); print(fig)
```

Your estimate of the final sample size is much lower after getting all this bad news about accrual rates. Notice also that the predictions have much less variability. This is partly due to your greater knowledge of accrual when you combine your prior distribution with the actual data and partly due to the fact that you are predicting less far into the future.

## Post mortem analysis

For this trial, you were able to get `r n_final` patients, but it took a lot longer than you expected, `r t_final` days instead of `r T` days. Time to get ready for your next clinical trial. NOT SO FAST! You are not done with simulations when the trial is over. After the trial, take a look at how your accrual rate ranks relative to the range of accrual rates associated with your prior distribution.

## Figure 3.6. Comparison of final accrual rate to prior distribution.

```{r gamma-poisson-post, fig.width=10, fig.height=1, eval=TRUE}
load("fig/3.6a.RData"); print(fig)
load("fig/3.6b.RData"); print(fig)
```

Okay, so you were way off. Maybe you just had a bad day. But let's look at your other clinical trials.

## Figure 3.7. Percentile plot showing bias to low end.

```{r gamma-poisson-percentile1, fig.width=10, fig.height=1}
load("fig/3.7.RData")
print(fig)
```

When you are estimating accrual, a low percentile means that the final data from all of your clinical trials is in the lower tail of your prior distribution. So a plot where most or all of the percentiles fall below 50% means that you have a overly optimistic assessment of accrual before most of your trials.

## Figure 3.8. Percentile plot showing bias to extremes.

```{r gamma-poisson-percentile2, fig.width=10, fig.height=1, eval=TRUE}
load("fig/3.8.RData")
print(fig)
```

Another pattern to look for is a tendency to fall at either extreme. This means that your prior distributions are too narrow and you have too much confidence in your ability to assess accrual rates. Maybe in the future, you should choose wider prior distributions (smaller values of S).

## Figure 3.9. Percentile plot showing relatively even spread.

```{r percentile-plot3, fig.width=10, fig.height=1, eval=TRUE}
load("fig/3.9.RData")
print(fig)
```

Just for perspective, here is a what a percentile plot might look like if your prior distributions were reasonably accurate. You tend to see (albeit very roughly) about as many percentiles above and below 50% and about as many in the middle (between 25% and 75%) as you do outside the middle.

## Part 4. Early predictions

```{r preload}
load("fig/4.1.RData")
```

The commonly cited objection to informative priors is that they can bias the research. Whether that is true or not in a hypothesis testing framework is a matter of open debate. But using informative priors for operational characteristics of a study will not bias any resulting hypothesis tests.

While most Bayesian models use flat, non-informative, or only weakly informative prior distributions, when you are simulating the operational features of a clinical trial, you need to be bold and use strong informative prior distributions. There are three reasons for this.

First, if the best you can do for accrual rates for a clinical trial is a flat prior, what you are really saying is that you think you might see a couple of patients every day, or maybe a couple of patients every year, and you really don't have a strong belief of one of these accrual rates over another. Anyone who can only predict accrual rates across such a wide range is unqualified to run a clinical trial.

Second, if you run a simulation with a flat prior before the trial starts, you get results with an unrealisticly wide range.

Third, a strong prior distribution provides you with stable estimates early in the clinical trial. This is worth an illustration.

### Figure 4.1. Prediction band (25 and 75 percentiles) from flat prior.

```{r early-predictions-flat, fig.width=10, fig.height=5}
print(fig)
```

The concept of a flat prior is a bit tricky when you have an infinite range, like a gamma distribution has, but in this case it is a gamma(0.001, 0.001), a common choice for a non-informative gamma prior.

This is possibly an unfair example because the trial had a slow start. But it takes more than 100 days into the trial to get a decent estimate of the upper bound. The first three predictions are a measure of how unstable things are. On the first day, you have recruited one patient, and the 75th percentile is estimated at `r round(flat_predictions$hi[1]/365, 1)` years. On the second day, no one shows up, so your upper limit jumps to `r round(flat_predictions$hi[2]/365, 1)` years. On the third day, one patient shows up, so your upper limit plummets to `r round(flat_predictions$hi[3]/365, 1)` years.

Compare this to the predictions that use an informative prior. The posterior estimate in a Bayesian model with an informative prior is a weighted average of the prior distribution and the data, and that weighted average leans very heavily on the prior distribution early in the trial.

### Figure 4.2. Prediction band (25 and 75 percentiles) from strong prior.

```{r early-predictions-strong, fig.width=10, fig.height=5}
load("fig/4.2.RData"); print(fig)
```

### Figure 4.3. Prediction band (25 and 75 percentiles) from weak prior.

```{r early-predictions-weak, fig.width=10, fig.height=5, eval=TRUE}
load("fig/4.3.RData"); print(fig)
```

Even if you only have a very weak idea of what the accrual rate should be, you should still use that weak prior rather than a flat prior. This shows the behavior of a prior distribution with S=0.02, which puts equal weight on the prior and the data after 7 patients have entered the trial. It has a much more pessimistic view of accrual than the strong prior, because it is weighting the actual accrual data far more heavily. But the weak prior does not have the wild swings in predicition that the flat prior does.

You can look carefully at the last two plots and get a sense for when you might want a weak prior and when you might want a strong prior. There are several rules for when you might want to "hit the panic button" in a research study. A simple rule says to review your clinical trial and propose modifications if the probabilty of finishing after the target time increases from about 50% to about 75%.

For the weak prior distribution, this occurs on day 12. For the strong prior distribution, this occurs on day 33. If you've run this sort trial many times before, you really shouldn't react to a little bit of early bad news. But eventually, if enough bad news accumulates, you do need to react.

On the other hand, if this trial is novel in many ways and you have a lot of uncertainty about the true accrual rate, then you want to pay close attention to even a little bit of bad news and react quickly.

The strength of the prior distribution provides a balance between being sensitive to what the data is telling you about accrual but not overreacting.

## Part 5. Hedging priors

One problem with informative prior distributions is that many researchers get them wrong. They are either too optimistic about the accrual rate, or they think they know the accrual rate with more certainty than they actually do, or sometimes both.

You can minimize the problems associated with a bad informative prior by adding a hedging hyperparameter.

### Here's the code for a hedging hpyerparameter.

```{r hedging-code}
f <- "5-hedging-extension.stan"
cat(readLines(f), sep="\n")
```


### Figure 5.1. Comparison of simple and hedged predictions prior to data collection.

```{r hedging-prior, fig.width=10, fig.height=1}
load("fig/5.1a.RData"); print(fig)
load("fig/5.1b.RData"); print(fig)
```

The hedging hyperprior creates a mixture of gamma priors from weak to strong. So it is going to produce more variable predictions than a simple gamma prior.

### Figure 5.2. Updated prediction with a hedged prior.

```{r hedging-update-hedging, fig.width=10, fig.height=1}
load("fig/5.2a.RData"); print(fig)
load("fig/5.2b.RData"); print(fig)
load("fig/5.2c.RData"); print(fig)
```

The update with the hedging hyperprior ends up downweighting the strength of the prior distribution, so the weighted average of the data and the prior is weighted very heavily towards the data.

### Figure 5.3. Updated prediction with a simple prior.

```{r hedging-update-simple, fig.width=10, fig.height=1}
load("fig/5.3a.RData"); print(fig)
load("fig/5.3b.RData"); print(fig)
load("fig/5.3c.RData"); print(fig)
```

In contrast, because the simple prior was very strong, the weighted average of the prior and the data is pulled back towards the prior.

### Figure 5.4. Updated prediction with a flat prior.

```{r hedging-update-flat, fig.width=10, fig.height=1}
load("fig/5.4a.RData"); print(fig)
load("fig/5.4b.RData"); print(fig)
```

The flat prior behaves as you would suspect, putting pretty much all of the weight on the data.

### Figure 5.5. Updated distribution of hedging hyperparameter.

```{r hedging-update-hyperparameter, fig.width=10, fig.height=1}
load("fig/5.5.RData"); print(fig)
```

The distribution of the hyperparameter shows that when the data and the prior distribution disagree, the hyperparameter shrinks towards zero, effectively weakening the prior. 

### Alternative scenario

```{r preload-2}
load("fig/5.6a.RData")
```

Consider an alternative scenario, where the prior distribution and the actual accrual data are in close agreement. This might occur if the time to recruit `r dat_update2$n` patients was `r dat_update2$t` days instead of `r dat_update$t` days. This is an observed accrual rate of `r round(30*dat_update2$n/dat_update2$t, 1)` patients per month which is very close to the prior estimate of accrual rate of `r round(30*N/T)` patients per month.


### Figure 5.6. Alternative update with a hedged prior.

```{r hedging-update2-hedging, fig.width=10, fig.height=1}
load("fig/5.6a.RData"); print(fig)
load("fig/5.6b.RData"); print(fig)
load("fig/5.6c.RData"); print(fig)
```

The heding hyperprior does not need to downweight the strong prior when the data and the prior as in such close agreement.

### Figure 5.7. Alternative update prediction with a simple prior.

```{r hedging-update2-simple, fig.width=10, fig.height=1}
load("fig/5.7a.RData"); print(fig)
load("fig/5.7b.RData"); print(fig)
load("fig/5.7c.RData"); print(fig)
```

The predicted sample size of the simple prior is very similar to the hedging hyperprior when you have such good agreement.

### Figure 5.8. Alternative update with a flat prior.

```{r hedging-update2-flat, fig.width=10, fig.height=1}
load("fig/5.8a.RData"); print(fig)
load("fig/5.8b.RData"); print(fig)
```

The flat prior also is close to the observed accrual rate, but because it does not add the precision of the prior to the precision of the data, you see much more variation in the predicted total sample size.

### Figure 5.9. Alternative update of hedging hyperparameter.

```{r hedging-update2-hyperparameter, fig.width=10, fig.height=1}
load("fig/5.9.RData"); print(fig)
```

The hyperparameter does not shrink as it did in the earlier case, and in fact, it has expanded slightly, with a mean and median both a bit larger than 1. This is a reward for choosing such a good prior distribution, but if you want to, you can modify this model to insure that the average weight given to the prior distribution never exceeds 1, even if your prior matches the data perfectly.

### Part 6. Simulating delays in a clinical trial.

There are several different models for accrual delays. They may or may not include a start-up time before ANY accrual, and they may or may not include a warm-up time early in the trial when accrual is slower. If there is a warm-up time, there are several ways to model the transition from slow accrual to full accrual.


### Figure 6.1. Discontinuous transition from slow to normal accrual.
 
```{r delay-example1, fig.height=2, fig.width=5}
load("fig/6.0.RData")
load("fig/6.1.RData"); print(fig)
```

Here's a discontinuous transition from slow to full accrual. It has three parameters, the duration of the waiting to start period, the rate during slow accrual, and the duration of the transition to full accrual.

### Figure 6.2. Linear transition from slow to normal accrual.

```{r delay-example2, fig.height=2, fig.width=5}
load("fig/6.2.RData"); print(fig)
```

Here's a linear transition from slow to full accrual. It has two parameters, the duration of the waiting to start period and the duration of the transition to full accrual.

### Figure 6.3. Linear transition from fast to normal accrual.

```{r delay-example3, fig.height=2, fig.width=5}
load("fig/6.3.RData"); print(fig)
```

Here's an example of a partially discontinuous transition where the early accrual is fast, perhaps because there were a large number of patients waiting for the trial to start.

### Figure 6.4. Smooth transition from slow to normal accrual.

```{r delay-example4, fig.height=2, fig.width=5}
load("fig/6.4.RData"); print(fig)
```

Here's a smooth transition using a rescaled version of the function $f(x)=3x^2-2x^3$. This functiton is a cubic spline with the properties

f(0)=0 (starts at zero to maintain continuity at 0)

f'(0) (starts out flat to maintain smoothness at 0)

f(1)=1 (rises to a new level)

f'(1)=0 (levels off at the new level to maintain smoothness)

### Priors for a delay model

We'll fit the discontinuous model, the first one shown, because it is convenient, but there are probably very few practical differences among these models. The discontinuous model has three parameters. Delta1 represents the duration of time waiting for the trial to start as a fraction of the total time planned. Delta2 represents the ratio of the accrual rate during the slow accrual period and the accrual rate during the full accrual period. Delta3 represents the duration of time for the slow accrual period as a fraction of the total time planned.

Generally, you should use strong priors here to limit delta1 and delta3 to the lower end of the range from 0 to 1. If you really thought that delta1 and/or delta3 would be much closer to 1, then you should rethink what your total time planned should be.

### Figure 6.5. Prior distribution for delta1 (time waiting to start).

```{r delay-priors1, fig.height=1, fig.width=10}
load("fig/6.5a.RData"); print(fig)
load("fig/6.5b.RData"); print(fig)
load("fig/6.5c.RData"); print(fig)
```

Here's some possible prior distributions. For delta1, the proportion of the total planned time that you spend waiting for the study to get started, a beta(`r N*D1*S1`, `r N*S1*(1-D1)`) corresponds to an expectation that `r D1*100`% of the time (`r D1*T` days) will be lost on average before the trial even begins. The weight assigned to this prior corresponds to `r S1*100`% of the total sample size.

### Figure 6.6. Prior distribution for delta2 (accrual rate early), rescaled to patients per month.

```{r delay-priors2, fig.height=1, fig.width=10}
load("fig/6.6a.RData"); print(fig)
load("fig/6.6b.RData"); print(fig)
load("fig/6.6c.RData"); print(fig)
```

For delta2, the proportionate reduction in accrual during the warm-up period, a beta(`r S2*N*D2`, `r S2*N*(1-D2)`) corresponds to an expectation that the reduction during warm-up will be about `r 100*D2`% of the regular accrual rate on average, or roughly `r round(D2*30*N/T)` patients per month. The weight assigned to this prior corresponds to `r S2*100`% of the total sample size.

### Figure 6.7. Prior distribution for delta3 (duration of early accrual), rescaled to days.

```{r delay-priors3, fig.height=1, fig.width=10}
load("fig/6.7a.RData"); print(fig)
load("fig/6.7b.RData"); print(fig)
load("fig/6.7c.RData"); print(fig)
```

For delta3, the proportion of the total planned time that you spend in the warm-up period, a beta(`r S3*N*D3`, `r S3*N*(1-D3)`) corresponds to an expectation that `r 100*D3`% of the time your trial will be stuck in the warm-up phase. The weight assigned to this prior corresponds to `r S3*100`% of the total sample size.

### Here's the code to fit a simulation involving delays.

```{r delay_code}
f <- "6-beta-test3.jags"
cat(readLines(f), sep="\n")
```

The delay model involves discontinuities, which cause problems for stan, so this code is for a similar program called jags.

### Figure 6.8. Prior estimate of total sample size, accounting for delays.

```{r delay-fit0, fig.height=1, fig.width=10}
load("fig/6.8.RData"); print(fig)
```

You are taking a big hit here. But you should be thankful that you're accounting for these sorts of things before the trial starts.

You should consider exploring how sensitive your estimated total sample size is relative to each of the delay parameters.

### Figure 6.9. Effect of prior parameters on estimated total sample size.

```{r delay-scaterplots, fig.width=4, fig.height=4}
load("fig/6.9a.RData"); print(fig)
load("fig/6.9b.RData"); print(fig)
load("fig/6.9c.RData"); print(fig)
load("fig/6.9d.RData"); print(fig)
```

It looks like the proportion of time waiting to start has a far greater influence on the total sample size than the other parameters. This would change if you changed your prior distributions, of course, but in general it makes sense. Time lost waiting for the trial to start is time when no subjects enter. Time during slow accrual is important but at least you are getting some subjects in during this time frame. The relative slowness during the warm-up period is also not too critical.

The actual trial that I have used so far did not have a formal model for delays, but it is easy enough to modify the data to fit this example. You need to prepend the data with a string of zeros corresponding to the days when the trial was waiting to get started.

### Updating the delay simulation--150 days of nothing.

Suppose that you are on day `r t1`, and not a single patient has shown up. You were expecting the time to the first patient to be around day `r round(D1*T)`, but it looks like things might be far worse. What does the above average delay to the start of your study do to your estimate of the total sample size?

### Figure 6.10. Updated estimate of total sample size after 150 days of nothing.

```{r delay-fit1, fig.height=1, fig.width=10}
load("fig/6.10a.Rmd"); print(fig)
load("fig/6.10b.Rmd"); print(fig)
```

It looks like the delay has not hurt the trial that much so far.

### Figure 6.11. How much longer do you expect to wait after 150 days of nothing.

```{r delay-updated_start, fig.width=10, fig.height=1}
load("fig/6.11a.RData"); print(fig)
load("fig/6.11b.RData"); print(fig)
```

This boxplot shows when you think the trial will start, now that you know that the delay is at least 150 days. Notice that the model is placing a small probability on the possibility that the study actually started prior to 150 days, but nobody wanted to announce the start until some patients actually started showing up. You can easily modify this model to zero out the probability associated with start times earlier than 150 days.

The remaining parameters remain unchanged, of course.

### Figure 6.12. Update of delta2 after 150 days of nothing.

```{r delay-update-delta2, fig.width=10, fig.height=1}
load("fig/6.12a.RData"); print(fig)
load("fig/6.12b.RData"); print(fig)
```

The slight differences you see here are just variations due to the simulation and the prior estimate of the accrual rate during the early phase and the updated estimate of the accrual rate during the early phase are identical if you don't have any data yet in the early phase.

### Figure 6.13. Update of delta3 after 150 days of nothing.

```{r delay-update-delta3, fig.width=10, fig.height=1}
load("fig/6.13a.RData"); print(fig)
load("fig/6.13b.RData"); print(fig)
```

Similarly, the estimated duration of the early phase is unchanged if you haven't even entered the early phase yet.

### Figure 6.14. Update of lambda after 150 days of nothing.

```{r delay-update-lambda, fig.width=10, fig.height=1}
load("fig/6.14a.RData"); print(fig)
load("fig/6.14b.RData"); print(fig)
```

Finally, the estimated accrual rate during the late phase is unchanged. You have no data in the early or late phases, yet.

### Update 2. We have 240 days of accrual data after 192 days of waiting.

On day 192 (finally!) the trial starts accruing patients. The early phase, as expected, was slow, but now you have `r n2` patients after `r t2-t1` days of active accrual. So how are you doing now?

### Figure 6.15. Second update of total sample size.

```{r delay-fit2, fig.height=1, fig.width=10}
load("fig/6.15a.RData"); print(fig)
load("fig/6.15b.RData"); print(fig)
```

The news is very bad. What's accounting for most of your shortfall?

### Figure 6.16. Second update of delta1.

```{r delay-update2-delta1, fig.width=10, fig.height=1}
load("fig/6.16a.RData"); print(fig)
load("fig/6.16b.RData"); print(fig)
```

The model still clings stubbornly to the belief that the researchers were not honest with the stated start date, and that there were a few days of zero accrual early on.

### Figure 6.17. Second update of delta2.

```{r delay-update2-delta2, fig.width=10, fig.height=1}
load("fig/6.17a.RData"); print(fig)
load("fig/6.17b.RData"); print(fig)
load("fig/6.17c.RData"); print(fig)
```

The rate of accrual during the early phase appears to be only slightly below what we thought it might be.

### Figure 6.18. Second update of delta3.

```{r delay-update2-delta3, fig.width=10, fig.height=1}
load("fig/6.18a.RData"); print(fig)
load("fig/6.18b.RData"); print(fig)
load("fig/6.18c.RData"); print(fig)
```

The duration of the early phase, however, appears to be substantially longer than we expected.


### Figure 6.19. Second update of lambda.

```{r delay-update2-lambda, fig.width=10, fig.height=1}
load("fig/6.19a.RData"); print(fig)
load("fig/6.19b.RData"); print(fig)
load("fig/6.19c.RData"); print(fig)
```

The accrual rate during the late phase seems to be a bit lower than we thought, but not by much. It looks like no one factor is totally to blame. The shortfall is affected by the longer than normal time to start the study, combined with the longer than normal duration of slow accrual combined with the slower than expected accrual rates both early and late.

### Part 7. Modeling exclusions and refusals

There's a gauntlet that patients must travel through before they are officially part of your clinical trial. As you ask them questions, you will find that some of the patients might not meet all of your inclusion criteria. Also, the patients may change their minds, after hearing the details of your clinical trial, and decide that they don't really want to participate after all.

You can model these two processes using binomial distributions.

Here's some possible prior distributions for the proportion excluded and the proportion refusing consent.

### Figure 7.1. Prior distribution for pi1, proportion excluded.

```{r binomial-priors1, fig.height=1, fig.width=10}
load("fig/7.0.RData")
load("fig/7.1a.RData"); print(fig)
load("fig/7.1b.RData"); print(fig)
load("fig/7.1c.RData"); print(fig)
```

A prior strength of 0.2 looks reasonable for the proportion excluded.

### Figure 7.2. Prior distribution for pi2, proportion refusing.

```{r binomial-priors2, fig.height=1, fig.width=10}
load("fig/7.2a.RData"); print(fig)
load("fig/7.2b.RData"); print(fig)
load("fig/7.2c.RData"); print(fig)
```

A prior strength of 0.1 looks reasonable for proportion refusing.

If you do a simple extrapolation, the total sample size after exclusions and refusals should be about `r 100*(1-P1)*(1-P2)`% of the `r N` planned patients or `r N*(1-P1)*(1-P2)` patients.

### Here's what the code would look like for this model.

```{r binomial-code}
f <- "7-binomial-extension.stan"
cat(readLines(f), sep="\n")
```

### Figure 7.3. Prior estimate of sample size, accounting for exclusions and refusals.

```{r binomial-prior-n, fig.height=1, fig.width=10}
load("fig/7.3a.RData"); print(fig)
load("fig/7.3b.RData"); print(fig)
load("fig/7.3c.RData"); print(fig)
```

You should consider exploring how sensitive your estimated total sample size is relative to each of the binomial parameters.

### Figure 7.4. How the prior distributions affect the estimated total sample size.

```{r binomial-scatterplots, fig.width=4, fig.height=4}
load("fig/7.4a.RData"); print(fig)
load("fig/7.4b.RData"); print(fig)
```

The estimated total sample size is more sensitive to the proportion consenting, mostly because your prior beliefs gave this proportion a broader range of possibilities.

### Update. 

The trial that you have seen illustrated thus far did not track exclusions and refusals, but let's suppose that there were `r n1` patients who showed up, but `r n1-n2` were excluded and `r n2-n3` who refused consent, leaving `r n3` available for randomization.

### Figure 7.5. Updated sample size before exclusions and refusals.

```{r binomial-update-n1, fig.width=10, fig.height=1}
load("fig/7.5a.RData"); print(fig)
load("fig/7.5b.RData"); print(fig)
```

Your accrual rate is better than you expected, at least for getting people to show up.

### Figure 7.6. Updated sample size after exclusions, before refusals.

```{r binomial-update-n2, fig.width=10, fig.height=1}
load("fig/7.6a.RData"); print(fig)
load("fig/7.6b.RData"); print(fig)
```

The exclusions are about what you'd expect, and that means you're still ahead of schedule at this point.

### Figure 7.7. Updated estimates after exclusions and refusals

```{r binomial-update-n3, fig.width=10, fig.height=1}
load("fig/7.7a.RData"); print(fig)
load("fig/7.7b.RData"); print(fig)
```

The refusals, though, are a disaster. You are losing a lot more than `r 100*P2`% patients due to refusals. In fact, it looks like the surge in the proportion of refusals is the main factor influencing your behind-schedule numbers.

## Conclusion

An informative prior distribution allows you to model the operational characteristics of a clinical trial before it starts.

As your trial progresses, you combine the informative prior distribution with observed data to get an updated prediction.

A strong prior distribution will prevent you from panicking from a small amount of bad news early in the trial.

A weak prior distribution will allow you to react quickly to early evidence of problems.

A flat or non-informative prior leads to unstable predictions early in the trial.

When your trial is done, compare the final results to the prior.

---
title: "lognormal"
author: "Steve Simon"
date: "May 3, 2017"
output: html_document
---

```{r preliminaries, echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
library(cowplot)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(magrittr)
library(quantreg)
library(rstan)
library(tidyr)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
opts_chunk$set(
  echo=TRUE,
  message=FALSE,
  warning=FALSE)
```

```{r lognormal, eval=FALSE}
f <- "lognormal.stan"
dat <- list(N=350, T=1095, S=0.5, S1=0.5, GSD=2, M=5)
fit_ln <- stan(file=f,
  data=dat, iter= 1000, chains = 4)
tidy(fit_ln)

fit_ln               %>%
  as.data.frame      %>%
  mutate(i="before") -> sim_ln

head(sim_ln)
# Boxplot of total sample size
sim_ln                                  %>%
  select(starts_with("eta"))            %>%
  gather(key, value, starts_with("eta")) %>%
  ggplot(aes(key, value))                 +
  expand_limits(y=0)                      +
  ylab("Estimated total sample size")     + 
  xlab(" ")                               +
  geom_boxplot()                          +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                   +
  geom_hline(yintercept=1, color="gray")  +
  geom_hline(yintercept=exp(0.5*log(dat$GSD)^2), color="gray")  +
  coord_flip()

```

I wanted to use the lognormal distribution to simulate a random effect that is multiplicative, but it is a bit tricky to do this in practice. I needed to review some fundamental properties of the lognormal distribution to get it right.

The lognormal distribution is a distribution where the log is normal. More formally, X is lognormally distributed if log(X) is normally distributed. Inverting that function, you can also say that if Y is normally distributed, then exp(Y) is lognormally distributed.

While the normal distribution is symmetric, the lognormal distribution is skewed to the right (skewed positive), meaning that it tends to produce has outliers on the right more often than on the left. If the value of $\sigma$ in the normal distribution is large, the lognormal distribution will end up extrememely skewed.

You can generate a random sample of lognormal variables in two different ways. You can compute a random sample of normal variables (using rnorm) and then exponentiating the values, or you can generate the lognormal values directly using the rlnorm function.

```{r simple}
y <- rnorm(1000, 0, 1)
x1 <- exp(y)
lb1 <- "exp(rnorm)"
x2 <- rlnorm(1000, 0, 1)
lb2 <- "rlnorm"
data.frame(x=c(x1, x2), lb=c(lb1, lb2)) %>%
  ggplot(aes(lb, x))                     +
  ylab(" ")                              + 
  xlab(" ")                              +
  geom_boxplot()                         +
  stat_summary(fun.y="mean", 
               geom="point", 
               size=4,
               pch="+")                  +
  coord_flip()
```

Notice that even with a standard deviation of only 1, the lognormal distribution is quite skewed.

Like most other packages, R defines the lognormal distribution in terms of the parameters $\mu$ and $\sigma$ of the underlying normal distribution. But you can also define the lognormal distribution in terms of the geometric mean (GM) and the geometric standard deviation (GSD) with

GM = $e^\mu$ and GSD = $e^\sigma$.

Just for reference, since you know the probability density function of the normal distribution, it is not hard to calculate the probability density function of the lognormal distribution.

The fundamental change of variable approach requires a monotone function (or requires a few tedious corrections). Thankfully exp() is a monotone function. The general result is that for a random variable Y=g(X) where X has pdf $f_X(x)$, the density function of Y, $f_Y(y)$, is equal to

$\lvert {d \over dy} g^{-1}(y) \rvert f_X(g^{-1}(y))$ 

The first half of this expression is the dreaded Jacobian. But since $g^{-1}$ is just the natural logarithm, the first half end up being $1 \over y$. The density of the normal distribution is

${1 \over \sigma \sqrt{2*\pi}} e^{-{(x-\mu)^2} \over {2 \sigma^2}}$

so just replace x with ln(y) and stick in the Jacobian $1 \over y$ to get

${1 \over y \sigma \sqrt{2*\pi}} e^{-{(ln(y)-\mu)^2} \over {2 \sigma^2}}$

I mention this because that Jacobian is needed at times with Bayesian models.

Here's what the density function looks like for a lognormal(0, 1) distribution.

```{r density}
x <- seq(0, 10, length=1000)
y <- dlnorm(x, 0, 1)
data.frame(x, y) %>%
  ggplot(aes(x, y)) +
  xlab(" ") +
  ylab("lognormal density") +
  geom_line()
```

So my thought was to generate a single rate based on the gamma distribution,

$\lambda$ ~ Gamma(N*S, N*T),

where N, S, and T are constants provided elsewhere. Then you create lognormal disturbances

$\eta_i$ ~ lognormal(0, $\sigma$)

and compute rates for each center in a multi-center trial as

$\lambda~\eta_i$

Fair enough. By making the mean be zero on the normal, that's the same (so I thought) to having a mean of 1 on the lognormal scale. A large value of $\sigma$ would produce large variations in the rates from one center to another and a small value of $\sigma$ would produce more consistency in the rates across centers.


This is a test $\alpha$.
